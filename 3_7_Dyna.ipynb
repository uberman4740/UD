{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dyna-Q Big Picture](#Dyna-Q Big Picture)<br>\n",
    "\n",
    "[Learning T](#Learning T)<br>\n",
    "[How to Evaluate T?](#How to Evaluate T?)<br>\n",
    "\n",
    "[Learning R](#Learning R)<br>\n",
    "\n",
    "[Dyna Q recap](#Dyna Q recap)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro**\n",
    "\n",
    "One problem with q-learning is that it takes many experienced tuples to converge. <font color = 'lightgrey'>This is expensive in terms of interacting with the world because you have to take a real step. In other words executed trade in order to gather information. \n",
    "</font>\n",
    "\n",
    "To address this problem Rich Sutton invented Dyna. Dyna works by building models of T (the transition matrix) and R (the reward matrix).\n",
    "\n",
    "Then after each real interaction with the world we hallucinate many additional interactions, usually a few hundred that are used then to update the Q-table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna-Q Big Picture <a id='Dyna-Q Big Picture'></a>\n",
    "\n",
    "[Dyna-Q Big Picture](#Dyna-Q Big Picture)<br>\n",
    "> Dina-Q is an algorithm intended to speed up the learning or model convergence for q-learning. \n",
    "\n",
    "**Remember that q-learning is model-free, meaning that it does not rely on T or R. $T$ being our transition matrix and $r$ being our reward function. It doesn't know $T$ or $r$.** Dyna ends up becoming a blend of model-free and model-based methods. Here's how it works. \n",
    "\n",
    "<img src='pics/Snap384.jpg' alt='Drawing' style='width: 250pt;'/>\n",
    "\n",
    "So let's first consider plain old q-learning. Dyna is really in addition to q-learning, so let's start with the regular q-learning and then look at how we add Dyna into it. \n",
    "\n",
    "* so we initialize our Q table and then we begin iterating \n",
    "\n",
    "* we observe s, we execute action a and then we observe our new state s prime and our reward R.\n",
    "\n",
    "* We then update our q table with this experience tuple and repeat. We do that over and over and over again interacting with the world and our q table becomes better and better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmenting our Q-learner\n",
    "\n",
    "<img src='pics/Snap386.jpg' alt='Drawing' style='width: 300pt;'/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color = 'green'>**So when we augment q-learning with dyna-Q we add three new components:** </font>\n",
    "\n",
    "1. the first is that we add some logic that enables us to learn models of T and R. \n",
    "\n",
    "* Then, <font color = 'lightgrey'>for lack of a better term</font>, we hallucinate an experience. \n",
    "\n",
    "    * So rather than interacting with the real world like we do up here with the q-learning part <font color = 'lightgrey'>(and this is expensive by the way)</font>, we hallucinate these experiences, update our q table and repeat this many times, maybe hundreds of times. \n",
    "    \n",
    "        * This operation is very cheap compared to interacting with the real world. So we can leverage the experience we gained here from interacting with the real world (points to Q-learn update line), but then update our model more completely before we step out and interact with the real world again. \n",
    "        \n",
    "* After we've iterated enough times down here maybe 100 maybe two hundred times, we then return back up here and resume our interaction with the real world. \n",
    "        \n",
    "> <font color = 'green'>The key thing here is that for each experience with the real world we have maybe 100 or 200 updates of our model. </font>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at each of these components in a little more detail. **\n",
    "\n",
    "So in the part where we update our model (box labeled learn model T, R), what we really want to do is find new values for T and R. \n",
    "\n",
    "<img src='pics/Snap388.jpg' alt='Drawing' style='width: 200pt;'/>\n",
    "\n",
    "The point where we update our model includes the following: we want to update **T (I'm calling it a prime for the moment) which represents our transition matrix and we want to update our reward function. **\n",
    "\n",
    "* **Now remember that T is the probability that if we're in state s and we take action a, we'll end up in s prime**; \n",
    "* r is our expected reward if we're in state s and we take action a.\n",
    "\n",
    "<font color = 'lightgrey'>I'll show you in a moment how we'll update T & R. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'green'>**how we hallucinate an experience. ** </font>\n",
    "\n",
    "1. **First, we randomly selected an s and an a**. So our state and action are chosen totally at random.\n",
    "\n",
    "* then we infer our new state (s') by looking at T, and then we infer our immediate reward (r) by looking at R[s,a] (our R table). <img src='pics/Snap389.jpg' alt='Drawing' style='width: 300pt;'/>\n",
    "\n",
    "* So now we've got s, a, s', and r for a complete experience tuple. We can update our q table using that. So the q table update is our final step and this is really all there is to Dyna Q.\n",
    "\n",
    "We've added these three sections and what's missing and I'll get to in just a moment is how do we update our model of T and our model of R "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning T <a id='Learning T'></a>\n",
    "\n",
    "[Learning T](#Learning T)<br>\n",
    "<font color = 'lightgrey'>so when I'm about to show you are my methods for doing these things they may not correspond exactly to what Rich Sutton did so keep that in mind. But I've used these methods in practice and tell them to work really well</font>\n",
    "\n",
    "We'll start with learning T. **Remember T[s,a,s'] represents the probability that if we are in state s and we take action a, we will end up in state s'.**\n",
    "\n",
    "To learn a model of T, we're just going to observe how these transitions occur. So in other words we'll have an experience with the real world. We'll get back an s'. And we'll just count, how many times did it happen. \n",
    "\n",
    "<img src='pics/Snap391.jpg' alt='Drawing' style='width: 300pt;'/>\n",
    "\n",
    "So I am gonna introduce a new table called T count or $T_c$ and it goes like this:\n",
    "\n",
    "* so we initialize all of our t count values to be a very very small number. \n",
    "    <font color = 'lightgrey'>* The reason for that is later on you'll see that if we don't do that we could get in a situation where we have a divide by zero. </font>\n",
    "\n",
    "* **Then we begin executing q learning. And each time we interact with the real world, we observe s, a, s'; and then we just increment that location in our t count matrix. **\n",
    "><font color = 'green'>So every time we see a transition from s to s' with action a, we add a one to $T_c$. and that's pretty it's pretty simple that's it </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## How to Evaluate T? <a id='How to Evaluate T?'></a>\n",
    " \n",
    " [How to Evaluate T?](#How to Evaluate T?)<br>\n",
    " \n",
    "ok so now we've been watching our interactions with the real world for a while and we would like to consult our model of T. I would like for you to write an expression for T in terms of $T_c$ or T count.\n",
    "<br>\n",
    "<br>\n",
    "<font color = 'purple'>ok so fill in this text box with your opinion of how we can compute T[s,a,s']  in terms of $T_c$. Remember to $T_c$ is just the number of times each one of these has occurred </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/Snap392.jpg' alt='Drawing' style='width: 300pt;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider that state s and a are somewhat fixed, what we're trying to find is given s and a, what's the probability of a particular s'. **\n",
    "\n",
    "So we'll begin this by just consulting how many times did this transition occur: \n",
    "**$T_c[s,a,s']$**\n",
    "\n",
    "* In other words**<font color = 'green'>, how many times, were we in state s did action a result in s'. </font>**\n",
    "\n",
    "* Now we just need to divide **$T_c[s,a,s']$** by the total number of times we were in state s and executed action a. So essentially it's the sum over i, where we have i iterate over all the possible states of T[s,a,i]. \n",
    "\n",
    "    * **$\\sum\\limits_i T[s,a,i]$  is the number of times in total that we were in state s and executed action a.**\n",
    "    \n",
    "> $\\frac{T_c[s,a,s']}{\\sum\\limits_i T[s,a,i]}$ is the probability that we'll end up in s' if we're in state s and take action a. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning R <a id='Learning R'></a>\n",
    "\n",
    "[Learning R](#Learning R)<br>\n",
    "<img src='pics/Snap393.jpg' alt='Drawing' style='width: 300pt;'/>\n",
    "\n",
    "**The last step is how do we learn a model for R. **\n",
    "\n",
    "* Remember when we execute an action a in state s, we get an immediate reward little r. So again big <span style='background-color: lightgreen'>R[s,a]</span> is our expected reward, if we're in state s and we execute action a. \n",
    "\n",
    "* <span style='background-color: lightgreen'>little r</span> is our immediate reward when we experienced this in the real world.\n",
    "\n",
    "**So big R is our model, little r is what we get in an experience tuple.** So we want to update this model every time we have a real experience. And it's a simple equation, very much like the q table update equation. \n",
    "\n",
    "* We have $(1-\\alpha)R[s,a]$ \n",
    "<font color = 'lightgrey'>* where $\\alpha$ is our learning rate and again that can typically be something like 0.2. We multiply that times our current value for R[s,a]. </font>\n",
    "\n",
    "* And then we add in our new estimate of what that value ought to be. And we just use r for the new estimate. \n",
    "\n",
    "So its alpha times little r, which is our immediate reward for our new best estimate of what the value should be, plus what the value was before, times $(1-\\alpha)$. \n",
    "\n",
    "So we're weighting <font color = 'lightgrey'>presumably</font> our old value more than our new value, so we converge more slowly. That's a simple way to build a model of R from observations of interactions with the real world.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna Q recap <a id='Dyna Q recap'></a>\n",
    "\n",
    "[Dyna Q recap](#Dyna Q recap)<br>\n",
    "\n",
    "<img src='pics/Snap394.jpg' alt='Drawing' style='width: 300pt;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we start with straight regular q-learning here and then we add three new components: \n",
    "\n",
    "1. update models of T and R, \n",
    "2. hallucinate an experience \n",
    "3. update our q table.\n",
    "    * we may repeat this many times, in fact maybe hundreds of times, until we're happy. Usually it's one or two hundred. Once we have completed those we then return back up to the top (<font color = 'red'>long red arrow</font>) and continue our interaction with the real world \n",
    "\n",
    "> the reason dyna-Q is useful is that these experiences with the real world are potentially very expensive and these hallucinations can be very cheap. When we iterate doing many of them we update our queue table much more quickly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The Dyna architecture consists of a combination of:\n",
    "\n",
    "* direct reinforcement learning from real experience tuples gathered by acting in an environment,\n",
    "* updating an internal model of the environment, and,\n",
    "* using the model to simulate experiences.\n",
    "\n",
    "<img src='pics/Dyna-architecture.png' alt='Drawing' style='width: 300pt;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference on Machine Learning, Austin, TX, 1990. [[link]](https://webdocs.cs.ualberta.ca/~sutton/papers/sutton-90.pdf)\n",
    "\n",
    "\n",
    "Sutton and Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998. [[link]](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)\n",
    "\n",
    "RL course by David Silver (videos, slides) [[link]](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "\n",
    "Lecture 8: Integrating Learning and Planning [[link]](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf)\n",
    "\n",
    "Sutton and Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998. [[link]](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
